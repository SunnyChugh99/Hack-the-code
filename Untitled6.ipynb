{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "281df674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"EXPERIMENT_DETAILS\"] = '{\"name\": \"testSAEEEEE\", \"algo_details\": {\"sklearn.ensemble.GradientBoostingClassifier\": null}, \"id\": \"383\", \"dataset\": \"Employee 1.csv\", \"target_column\": \"LeaveOrNot\"}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fafbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b8ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Experiment Execution with the following params:\n",
      "{\"name\": \"testSAEEEEE\", \"algo_details\": {\"sklearn.ensemble.GradientBoostingClassifier\": null}, \"id\": \"383\", \"dataset\": \"Employee 1.csv\", \"target_column\": \"LeaveOrNot\"}\n",
      "\n",
      "in encoding starts\n",
      "['Education', 'City', 'Gender', 'EverBenched'] columns are non numeric in feature dataset, encoding required.\n",
      "Columns identified to be encoded with label encoder: []\n",
      "Columns identified to be encoded with one hot encoder: ['Education', 'City', 'Gender', 'EverBenched']\n",
      "new one hot encoded df: [[1. 0. 0. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [1. 0. 0. ... 1. 0. 1.]]\n",
      "one hot encoder object: OneHotEncoder()\n",
      "\n",
      "final feature df created:       JoiningYear  PaymentTier  Age  ExperienceInCurrentDomain  \\\n",
      "0            2017            3   34                          0   \n",
      "1            2013            1   28                          3   \n",
      "2            2014            3   38                          2   \n",
      "3            2016            3   27                          5   \n",
      "4            2017            3   24                          2   \n",
      "...           ...          ...  ...                        ...   \n",
      "4648         2013            3   26                          4   \n",
      "4649         2013            2   37                          2   \n",
      "4650         2018            3   27                          5   \n",
      "4651         2012            3   30                          2   \n",
      "4652         2015            3   33                          4   \n",
      "\n",
      "      Education_Bachelors  Education_Masters  Education_PHD  City_Bangalore  \\\n",
      "0                     1.0                0.0            0.0             1.0   \n",
      "1                     1.0                0.0            0.0             0.0   \n",
      "2                     1.0                0.0            0.0             0.0   \n",
      "3                     0.0                1.0            0.0             1.0   \n",
      "4                     0.0                1.0            0.0             0.0   \n",
      "...                   ...                ...            ...             ...   \n",
      "4648                  1.0                0.0            0.0             1.0   \n",
      "4649                  0.0                1.0            0.0             0.0   \n",
      "4650                  0.0                1.0            0.0             0.0   \n",
      "4651                  1.0                0.0            0.0             1.0   \n",
      "4652                  1.0                0.0            0.0             1.0   \n",
      "\n",
      "      City_New Delhi  City_Pune  Gender_Female  Gender_Male  EverBenched_No  \\\n",
      "0                0.0        0.0            0.0          1.0             1.0   \n",
      "1                0.0        1.0            1.0          0.0             1.0   \n",
      "2                1.0        0.0            1.0          0.0             1.0   \n",
      "3                0.0        0.0            0.0          1.0             1.0   \n",
      "4                0.0        1.0            0.0          1.0             0.0   \n",
      "...              ...        ...            ...          ...             ...   \n",
      "4648             0.0        0.0            1.0          0.0             1.0   \n",
      "4649             0.0        1.0            0.0          1.0             1.0   \n",
      "4650             1.0        0.0            0.0          1.0             1.0   \n",
      "4651             0.0        0.0            0.0          1.0             0.0   \n",
      "4652             0.0        0.0            0.0          1.0             0.0   \n",
      "\n",
      "      EverBenched_Yes  \n",
      "0                 0.0  \n",
      "1                 0.0  \n",
      "2                 0.0  \n",
      "3                 0.0  \n",
      "4                 1.0  \n",
      "...               ...  \n",
      "4648              0.0  \n",
      "4649              0.0  \n",
      "4650              0.0  \n",
      "4651              1.0  \n",
      "4652              1.0  \n",
      "\n",
      "[4653 rows x 14 columns]\n",
      " encoding ends\n",
      "      JoiningYear  PaymentTier  Age  ExperienceInCurrentDomain  \\\n",
      "0            2017            3   34                          0   \n",
      "1            2013            1   28                          3   \n",
      "2            2014            3   38                          2   \n",
      "3            2016            3   27                          5   \n",
      "4            2017            3   24                          2   \n",
      "...           ...          ...  ...                        ...   \n",
      "4648         2013            3   26                          4   \n",
      "4649         2013            2   37                          2   \n",
      "4650         2018            3   27                          5   \n",
      "4651         2012            3   30                          2   \n",
      "4652         2015            3   33                          4   \n",
      "\n",
      "      Education_Bachelors  Education_Masters  Education_PHD  City_Bangalore  \\\n",
      "0                     1.0                0.0            0.0             1.0   \n",
      "1                     1.0                0.0            0.0             0.0   \n",
      "2                     1.0                0.0            0.0             0.0   \n",
      "3                     0.0                1.0            0.0             1.0   \n",
      "4                     0.0                1.0            0.0             0.0   \n",
      "...                   ...                ...            ...             ...   \n",
      "4648                  1.0                0.0            0.0             1.0   \n",
      "4649                  0.0                1.0            0.0             0.0   \n",
      "4650                  0.0                1.0            0.0             0.0   \n",
      "4651                  1.0                0.0            0.0             1.0   \n",
      "4652                  1.0                0.0            0.0             1.0   \n",
      "\n",
      "      City_New Delhi  City_Pune  Gender_Female  Gender_Male  EverBenched_No  \\\n",
      "0                0.0        0.0            0.0          1.0             1.0   \n",
      "1                0.0        1.0            1.0          0.0             1.0   \n",
      "2                1.0        0.0            1.0          0.0             1.0   \n",
      "3                0.0        0.0            0.0          1.0             1.0   \n",
      "4                0.0        1.0            0.0          1.0             0.0   \n",
      "...              ...        ...            ...          ...             ...   \n",
      "4648             0.0        0.0            1.0          0.0             1.0   \n",
      "4649             0.0        1.0            0.0          1.0             1.0   \n",
      "4650             1.0        0.0            0.0          1.0             1.0   \n",
      "4651             0.0        0.0            0.0          1.0             0.0   \n",
      "4652             0.0        0.0            0.0          1.0             0.0   \n",
      "\n",
      "      EverBenched_Yes  \n",
      "0                 0.0  \n",
      "1                 0.0  \n",
      "2                 0.0  \n",
      "3                 0.0  \n",
      "4                 1.0  \n",
      "...               ...  \n",
      "4648              0.0  \n",
      "4649              0.0  \n",
      "4650              0.0  \n",
      "4651              1.0  \n",
      "4652              1.0  \n",
      "\n",
      "[4653 rows x 14 columns]\n",
      "tar\n",
      "      LeaveOrNot\n",
      "0              0\n",
      "1              1\n",
      "2              0\n",
      "3              1\n",
      "4              1\n",
      "...          ...\n",
      "4648           0\n",
      "4649           1\n",
      "4650           1\n",
      "4651           0\n",
      "4652           0\n",
      "\n",
      "[4653 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/mlflow/data/dataset_source_registry.py:150: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment run for sklearn.ensemble.GradientBoostingClassifier with hyperparam: {'n_estimators': [100], 'learning_rate': [0.001, 0.01, 0.1, 0.5, 1.0], 'max_depth': range(1, 11), 'min_samples_split': range(2, 21), 'min_samples_leaf': range(1, 21), 'subsample': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
      "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]), 'max_features': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
      "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.12.1 of tpot is outdated. Version 0.12.2 was released Friday February 23, 2024.\n",
      "/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.8503895121540126\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.8526834201428143\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.8526834201428143\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.8526834201428143\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.8526834201428143\n",
      "\n",
      "Best pipeline: GradientBoostingClassifier(input_matrix, learning_rate=0.1, max_depth=8, max_features=0.35000000000000003, min_samples_leaf=19, min_samples_split=12, n_estimators=100, subsample=0.55)\n",
      "Pipeline(steps=[('gradientboostingclassifier',\n",
      "                 GradientBoostingClassifier(max_depth=8,\n",
      "                                            max_features=0.35000000000000003,\n",
      "                                            min_samples_leaf=19,\n",
      "                                            min_samples_split=12,\n",
      "                                            random_state=42, subsample=0.55))]) algorithms best selected by TPOT\n",
      "  score: 0.8445017182130584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.8445017182130584\n",
      "average_precision_score 0.6853586474585416\n",
      "f1_score 0.839360451875309\n",
      "precision_score 0.8452935148435857\n",
      "recall_score 0.8445017182130584\n",
      "roc_auc_score 0.8036061094032109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cloudpickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import (accuracy_score, average_precision_score, f1_score,\n",
    "                             precision_score, recall_score, roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow.sklearn\n",
    "from fosforio import get_local_dataframe, get_dataframe\n",
    "\n",
    "\n",
    "def experiment(exp_details, tpot_config, generations, population_size, cv, random_state, verbosity):\n",
    "    \"\"\"\n",
    "    exp_details: {\n",
    "        name: exp_name\n",
    "        id: exp_id\n",
    "        description: exp_description\n",
    "        dataset: exp_dataset\n",
    "        target_column: exp_target_column\n",
    "        algo_details: None\n",
    "    }\n",
    "    tpot_config: dict\n",
    "    generations: 0.5\n",
    "    population_size: 0.5\n",
    "    cv: 5\n",
    "    random_state: 42\n",
    "    verbosity: 2\n",
    "    \"\"\"\n",
    "    # Tracking URI set\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URL\", \"http://mlflow-server\"))\n",
    "\n",
    "    # Setting experiment name\n",
    "    mlflow.set_experiment(exp_details.get(\"name\", \"sample_experiment\"))\n",
    "\n",
    "    # Adding description to the experiment\n",
    "    tags = {'mlflow.note.content': exp_details.get(\"description\", \"sample_description\")}\n",
    "\n",
    "    # Reading input data\n",
    "    try:\n",
    "        if exp_details.get(\"dataset\").split(\".\")[-1].lower() in [\"csv\", \"tsv\", \"xlsx\", \"xls\"]:\n",
    "            # Read the input data file from /data mount attached to this notebook pod\n",
    "            input_file = \"/data/\" + exp_details.get(\"dataset\")\n",
    "            data = get_local_dataframe(input_file)\n",
    "        else:\n",
    "            # Input data is a dataset\n",
    "            data = get_dataframe(exp_details.get(\"dataset\"))\n",
    "            # Adding this to log input data in mlflow\n",
    "            input_file = '/tmp/input_data.csv'\n",
    "            data.to_csv(input_file, index=False)\n",
    "    except Exception as ex:\n",
    "        print(f\"Unable to read input dataset.\\nError: {ex}\")\n",
    "\n",
    "    # Data Preprocessing: Validating and encoding the data if required and imputing null values.\n",
    "    data = data.fillna(method='pad')  # Filling null values with the previous ones\n",
    "    data = data.fillna(method='bfill')  # Filling null value with the next ones\n",
    "    df_target, le_target, df_feature, le_dict_feature, oh_enc_feature, le_column_feature, oh_column_feature = encoding(\n",
    "        data, exp_details.get(\"target_column\"))\n",
    "\n",
    "    # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "    train_x, test_x, train_y, test_y = train_test_split(df_feature, df_target, train_size=0.75, test_size=0.25)\n",
    "\n",
    "    # Registering datasets with mlflow experiment run\n",
    "    dataset = mlflow.data.from_pandas(data, source=input_file)\n",
    "\n",
    "    for algo, hyperparam in exp_details.get(\"algo_details\").items():\n",
    "        if not hyperparam and type(hyperparam) != dict:\n",
    "            hyperparam = TPOTClassifier.default_config_dict[algo]\n",
    "        print(f\"Starting experiment run for {algo} with hyperparam: {hyperparam}\")\n",
    "        # Adding individual algorithm and its hyperparam to tpot config along with preprocessors and selectors\n",
    "        tpot_config_dict = {**tpot_config, **{algo: hyperparam}}\n",
    "        with mlflow.start_run(tags=tags):\n",
    "            pipeline_optimizer = TPOTClassifier(\n",
    "                generations=generations,\n",
    "                population_size=population_size,\n",
    "                cv=cv,\n",
    "                random_state=random_state,\n",
    "                verbosity=verbosity,\n",
    "                config_dict=tpot_config_dict\n",
    "            )\n",
    "\n",
    "            pipeline_optimizer.fit(train_x, train_y)\n",
    "\n",
    "            predicted_qualities = pipeline_optimizer.predict(test_x)\n",
    "\n",
    "            matrices = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "            print(f\"{str(pipeline_optimizer.fitted_pipeline_)} algorithms best selected by TPOT\")\n",
    "            print(f\"  score: {pipeline_optimizer.score(test_x, test_y)}\")\n",
    "\n",
    "            try:\n",
    "                # logging hyper params for the best run/pipeline chosen\n",
    "                for step, _ in pipeline_optimizer.fitted_pipeline_.named_steps.items():\n",
    "                    # Checking if the step used in TPOT pipeline is present in TPOT default config dict.\n",
    "                    # if yes, then log only the hyperparams which are present in TPOT default config and not all.\n",
    "                    step_name = [s for s in TPOTClassifier.default_config_dict.keys() if step.lower() in s.lower()]\n",
    "                    for k, v in pipeline_optimizer.fitted_pipeline_.named_steps[step].get_params().items():\n",
    "                        if step_name:\n",
    "                            if k in TPOTClassifier.default_config_dict.get(step_name[0]):\n",
    "                                mlflow.log_param(str(step) + \"_\" + str(k), v)\n",
    "                        else:\n",
    "                            mlflow.log_param(str(step) + \"_\" + str(k), v)\n",
    "            except Exception as ex:\n",
    "                print(f\"Exception occurred in logging params to mlflow.\\nex: {ex}\")\n",
    "\n",
    "            # logging model metric\n",
    "            for i in matrices:\n",
    "                if matrices[i]:\n",
    "                    mlflow.log_metric(i, matrices[i])\n",
    "                    print(i, matrices[i])\n",
    "            mlflow.log_metric(\"score\", pipeline_optimizer.score(test_x, test_y))\n",
    "\n",
    "            # Log input data to MLflow run artifact.\n",
    "            mlflow.log_artifact(input_file)\n",
    "\n",
    "            # Registering datasets with mlflow experiment run\n",
    "            mlflow.log_input(dataset, context=\"input\")\n",
    "\n",
    "            # Set custom tags\n",
    "            mlflow.set_tags({\n",
    "                \"template_id\": os.getenv(\"template_id\", \"sample_template_id\"),\n",
    "                \"notebook_name\": os.getenv(\"notebook_name\", \"sample_notebook_name\"),\n",
    "                \"algorithm\": algo,\n",
    "                \"algo_details\": exp_details.get(\"algo_details\"),\n",
    "                \"tpot_selected_algo\": str(pipeline_optimizer.fitted_pipeline_)\n",
    "            })\n",
    "\n",
    "            predictions = pipeline_optimizer.fitted_pipeline_.predict(train_x)\n",
    "            signature = infer_signature(train_x, predictions)\n",
    "\n",
    "            # Storing score function for the model\n",
    "            score_and_dump_func(\"/tmp/scoring_func\")\n",
    "            mlflow.log_artifact(\"/tmp/scoring_func\")\n",
    "\n",
    "            # Register the model\n",
    "            mlflow.sklearn.log_model(\n",
    "                pipeline_optimizer.fitted_pipeline_, \"model\",\n",
    "                registered_model_name=exp_details.get(\"name\", \"sample_experiment\"), signature=signature,\n",
    "                pip_requirements=['mlflow==2.10.0', 'sqlalchemy==1.3.5']\n",
    "            )\n",
    "\n",
    "            # Exporting the autogenerated code of tpot for best pipeline\n",
    "            pipeline_optimizer.export(f'tpot_exported_{exp_details.get(\"name\")}_{algo}.py')\n",
    "            mlflow.log_artifact(f'tpot_exported_{exp_details.get(\"name\")}_{algo}.py')\n",
    "\n",
    "\n",
    "def try_or(fn):\n",
    "    try:\n",
    "        out = fn()\n",
    "        return out\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def eval_metrics(y_actual, y_pred):\n",
    "    \"\"\"\n",
    "    :param\n",
    "    actual\n",
    "    pred\n",
    "    :returns\n",
    "    rmse, mae, r2\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"accuracy_score\": try_or(lambda: accuracy_score(y_actual, y_pred)),\n",
    "        \"average_precision_score\": try_or(lambda: average_precision_score(y_actual, y_pred)),\n",
    "        \"f1_score\": try_or(lambda: f1_score(y_actual, y_pred, average=\"weighted\", labels=np.unique(y_pred))),\n",
    "        \"precision_score\": try_or(\n",
    "            lambda: precision_score(y_actual, y_pred, average=\"weighted\", labels=np.unique(y_pred))),\n",
    "        \"recall_score\": try_or(lambda: recall_score(y_actual, y_pred, average=\"weighted\")),\n",
    "        \"roc_auc_score\": try_or(lambda: roc_auc_score(y_actual, y_pred))\n",
    "    }\n",
    "\n",
    "\n",
    "def score_and_dump_func(file_path):\n",
    "    \"\"\"\n",
    "    :param\n",
    "    file_path\n",
    "    \"\"\"\n",
    "\n",
    "    def score_func(model, request):\n",
    "        \"\"\"\n",
    "        :param\n",
    "        model\n",
    "        request\n",
    "        :returns\n",
    "        score_output\n",
    "        \"\"\"\n",
    "        # Enter your custom score function here\n",
    "\n",
    "        score_output = \"Success\"\n",
    "        return score_output\n",
    "\n",
    "    with open(file_path, \"wb\") as out:\n",
    "        cloudpickle.dump(score_func, out)\n",
    "\n",
    "\n",
    "def encoding(df, target_column):\n",
    "    \"\"\"\n",
    "    Checking whether encoding required in target and feature datasets.\n",
    "    If required, then encoding them with label and one hot encoding.\n",
    "    :param:\n",
    "    df: input dataframe\n",
    "    target_column: target column\n",
    "    :returns:\n",
    "    df_target: target dataframe\n",
    "    le_target: target label encoder object\n",
    "    df_feature: feature dataframe\n",
    "    le_dict_feature: dict of feature label encoder objects\n",
    "    oh_enc_feature: feature one hot encoder object\n",
    "    le_column_feature: list of feature label encoder columns\n",
    "    oh_column_feature: list of feature one hot encoder columns\n",
    "    \"\"\"\n",
    "    \n",
    "    print('in encoding starts')\n",
    "    df_target = df[[target_column]]\n",
    "    le_target = None\n",
    "    # Target column validation and encoding\n",
    "    if df.dtypes[target_column].name in ['object', 'bool']:\n",
    "        print(f\"target_column is of {df.dtypes[target_column].name} datatype, encoding required.\")\n",
    "        le_target = LabelEncoder()\n",
    "        df_target[target_column] = pd.DataFrame(le_target.fit_transform(df_target[target_column].astype(str)))\n",
    "        print(f\"Target column label encoded {df_target[target_column]}, object: {le_target}\")\n",
    "\n",
    "    # Feature column validation and encoding\n",
    "    df_feature = df.drop(target_column, axis=1)\n",
    "    non_numeric_cols = df_feature.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "    le_dict_feature = {}\n",
    "    le_column_feature = []\n",
    "    oh_column_feature = []\n",
    "    oh_enc_feature = None\n",
    "    if len(non_numeric_cols) >= 1:\n",
    "        print(f\"{non_numeric_cols} columns are non numeric in feature dataset, encoding required.\")\n",
    "        for col in non_numeric_cols:\n",
    "            if df_feature[col].nunique() >= 10:\n",
    "                le_column_feature.append(col)\n",
    "            else:\n",
    "                oh_column_feature.append(col)\n",
    "\n",
    "        print(f\"Columns identified to be encoded with label encoder: {le_column_feature}\\n\"\n",
    "              f\"Columns identified to be encoded with one hot encoder: {oh_column_feature}\")\n",
    "\n",
    "        # columns to be label encoded\n",
    "        if len(le_column_feature) == 0:\n",
    "            df_feature = df_feature\n",
    "        else:\n",
    "            for col in le_column_feature:\n",
    "                le_dict_feature[col] = LabelEncoder()\n",
    "                df_feature[col] = le_dict_feature[col].fit_transform(df_feature[col].astype(str))\n",
    "                print(f\"{col} column label encoded {df_feature[col]}, object: {le_dict_feature[col]}\")\n",
    "\n",
    "        # columns to be one hot encoded\n",
    "        if len(oh_column_feature) == 0:\n",
    "            df_feature = df_feature\n",
    "        else:\n",
    "            unique_combinations = pd.get_dummies(df_feature[oh_column_feature])\n",
    "            unique_combinations_list = unique_combinations.columns.tolist()\n",
    "            oh_enc_feature = OneHotEncoder()\n",
    "            oh_encoded_array = oh_enc_feature.fit_transform(df_feature[oh_column_feature]).toarray() if len(\n",
    "                oh_column_feature) > 1 else oh_enc_feature.fit_transform(df_feature[oh_column_feature]).toarray()\n",
    "            df_oh_enc = pd.DataFrame(oh_encoded_array, columns=unique_combinations_list)\n",
    "            df_feature = df_feature.drop(columns=oh_column_feature)\n",
    "            df_feature = df_feature.join(df_oh_enc)\n",
    "            print(f\"new one hot encoded df: {oh_encoded_array}\\n\"\n",
    "                  f\"one hot encoder object: {oh_enc_feature}\\n\")\n",
    "        print(f\"final feature df created: {df_feature}\")\n",
    "        print(' encoding ends')\n",
    "\n",
    "        print(df_feature)\n",
    "        df_feature.to_csv('/data/finalOutput.csv')\n",
    "        print('tar')\n",
    "        print(df_target)\n",
    "        \n",
    "    return df_target, le_target, df_feature, le_dict_feature, oh_enc_feature, le_column_feature, oh_column_feature\n",
    "\n",
    "\n",
    "# Adding Preprocessors and Selectors with tpot default hyperparameter tuning\n",
    "preprocessors_selectors = [\n",
    "    # Preprocessors\n",
    "    \"sklearn.preprocessing.Binarizer\",\n",
    "    \"sklearn.decomposition.FastICA\",\n",
    "    \"sklearn.cluster.FeatureAgglomeration\",\n",
    "    \"sklearn.preprocessing.MaxAbsScaler\",\n",
    "    \"sklearn.preprocessing.MinMaxScaler\",\n",
    "    \"sklearn.preprocessing.Normalizer\",\n",
    "    \"sklearn.kernel_approximation.Nystroem\",\n",
    "    \"sklearn.decomposition.PCA\",\n",
    "    \"sklearn.preprocessing.PolynomialFeatures\",\n",
    "    \"sklearn.kernel_approximation.RBFSampler\",\n",
    "    \"sklearn.preprocessing.RobustScaler\",\n",
    "    \"sklearn.preprocessing.StandardScaler\",\n",
    "    \"tpot.builtins.ZeroCount\",\n",
    "    \"tpot.builtins.OneHotEncoder\",\n",
    "    # Selectors\n",
    "    \"sklearn.feature_selection.SelectFwe\",\n",
    "    \"sklearn.feature_selection.SelectPercentile\",\n",
    "    \"sklearn.feature_selection.VarianceThreshold\",\n",
    "    \"sklearn.feature_selection.RFE\",\n",
    "    \"sklearn.feature_selection.SelectFromModel\"\n",
    "]\n",
    "tpot_config = {key: TPOTClassifier.default_config_dict[key] for key in preprocessors_selectors}\n",
    "\n",
    "# Running Experiment with user configured params.\n",
    "print(f\"Starting Experiment Execution with the following params:\\n{os.getenv('EXPERIMENT_DETAILS')}\\n\")\n",
    "experiment(exp_details=json.loads(os.getenv(\"EXPERIMENT_DETAILS\")), tpot_config=tpot_config, generations=5,\n",
    "           population_size=20, cv=5, random_state=42, verbosity=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf335b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
