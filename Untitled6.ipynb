{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15529834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"EXPERIMENT_DETAILS\"] = '{\"name\": \"testSAEEEEE\", \"algo_details\": {\"sklearn.ensemble.GradientBoostingClassifier\": null}, \"id\": \"383\", \"dataset\": \"Employee 1.csv\", \"target_column\": \"LeaveOrNot\"}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027d0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b13e7c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Experiment Execution with the following params:\n",
      "{\"name\": \"testSAEEEEE\", \"algo_details\": {\"sklearn.ensemble.GradientBoostingClassifier\": null}, \"id\": \"383\", \"dataset\": \"Employee 1.csv\", \"target_column\": \"LeaveOrNot\"}\n",
      "\n",
      "in encoding starts\n",
      "['Education', 'City', 'Gender', 'EverBenched'] columns are non numeric in feature dataset, encoding required.\n",
      "Columns identified to be encoded with label encoder: []\n",
      "Columns identified to be encoded with one hot encoder: ['Education', 'City', 'Gender', 'EverBenched']\n",
      "new one hot encoded df: [[1. 0. 0. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [1. 0. 0. ... 1. 0. 1.]]\n",
      "one hot encoder object: OneHotEncoder()\n",
      "\n",
      "final feature df created:       JoiningYear  PaymentTier  Age  ExperienceInCurrentDomain  \\\n",
      "0            2017            3   34                          0   \n",
      "1            2013            1   28                          3   \n",
      "2            2014            3   38                          2   \n",
      "3            2016            3   27                          5   \n",
      "4            2017            3   24                          2   \n",
      "...           ...          ...  ...                        ...   \n",
      "4648         2013            3   26                          4   \n",
      "4649         2013            2   37                          2   \n",
      "4650         2018            3   27                          5   \n",
      "4651         2012            3   30                          2   \n",
      "4652         2015            3   33                          4   \n",
      "\n",
      "      Education_Bachelors  Education_Masters  Education_PHD  City_Bangalore  \\\n",
      "0                     1.0                0.0            0.0             1.0   \n",
      "1                     1.0                0.0            0.0             0.0   \n",
      "2                     1.0                0.0            0.0             0.0   \n",
      "3                     0.0                1.0            0.0             1.0   \n",
      "4                     0.0                1.0            0.0             0.0   \n",
      "...                   ...                ...            ...             ...   \n",
      "4648                  1.0                0.0            0.0             1.0   \n",
      "4649                  0.0                1.0            0.0             0.0   \n",
      "4650                  0.0                1.0            0.0             0.0   \n",
      "4651                  1.0                0.0            0.0             1.0   \n",
      "4652                  1.0                0.0            0.0             1.0   \n",
      "\n",
      "      City_New Delhi  City_Pune  Gender_Female  Gender_Male  EverBenched_No  \\\n",
      "0                0.0        0.0            0.0          1.0             1.0   \n",
      "1                0.0        1.0            1.0          0.0             1.0   \n",
      "2                1.0        0.0            1.0          0.0             1.0   \n",
      "3                0.0        0.0            0.0          1.0             1.0   \n",
      "4                0.0        1.0            0.0          1.0             0.0   \n",
      "...              ...        ...            ...          ...             ...   \n",
      "4648             0.0        0.0            1.0          0.0             1.0   \n",
      "4649             0.0        1.0            0.0          1.0             1.0   \n",
      "4650             1.0        0.0            0.0          1.0             1.0   \n",
      "4651             0.0        0.0            0.0          1.0             0.0   \n",
      "4652             0.0        0.0            0.0          1.0             0.0   \n",
      "\n",
      "      EverBenched_Yes  \n",
      "0                 0.0  \n",
      "1                 0.0  \n",
      "2                 0.0  \n",
      "3                 0.0  \n",
      "4                 1.0  \n",
      "...               ...  \n",
      "4648              0.0  \n",
      "4649              0.0  \n",
      "4650              0.0  \n",
      "4651              1.0  \n",
      "4652              1.0  \n",
      "\n",
      "[4653 rows x 14 columns]\n",
      " encoding ends\n",
      "      JoiningYear  PaymentTier  Age  ExperienceInCurrentDomain  \\\n",
      "0            2017            3   34                          0   \n",
      "1            2013            1   28                          3   \n",
      "2            2014            3   38                          2   \n",
      "3            2016            3   27                          5   \n",
      "4            2017            3   24                          2   \n",
      "...           ...          ...  ...                        ...   \n",
      "4648         2013            3   26                          4   \n",
      "4649         2013            2   37                          2   \n",
      "4650         2018            3   27                          5   \n",
      "4651         2012            3   30                          2   \n",
      "4652         2015            3   33                          4   \n",
      "\n",
      "      Education_Bachelors  Education_Masters  Education_PHD  City_Bangalore  \\\n",
      "0                     1.0                0.0            0.0             1.0   \n",
      "1                     1.0                0.0            0.0             0.0   \n",
      "2                     1.0                0.0            0.0             0.0   \n",
      "3                     0.0                1.0            0.0             1.0   \n",
      "4                     0.0                1.0            0.0             0.0   \n",
      "...                   ...                ...            ...             ...   \n",
      "4648                  1.0                0.0            0.0             1.0   \n",
      "4649                  0.0                1.0            0.0             0.0   \n",
      "4650                  0.0                1.0            0.0             0.0   \n",
      "4651                  1.0                0.0            0.0             1.0   \n",
      "4652                  1.0                0.0            0.0             1.0   \n",
      "\n",
      "      City_New Delhi  City_Pune  Gender_Female  Gender_Male  EverBenched_No  \\\n",
      "0                0.0        0.0            0.0          1.0             1.0   \n",
      "1                0.0        1.0            1.0          0.0             1.0   \n",
      "2                1.0        0.0            1.0          0.0             1.0   \n",
      "3                0.0        0.0            0.0          1.0             1.0   \n",
      "4                0.0        1.0            0.0          1.0             0.0   \n",
      "...              ...        ...            ...          ...             ...   \n",
      "4648             0.0        0.0            1.0          0.0             1.0   \n",
      "4649             0.0        1.0            0.0          1.0             1.0   \n",
      "4650             1.0        0.0            0.0          1.0             1.0   \n",
      "4651             0.0        0.0            0.0          1.0             0.0   \n",
      "4652             0.0        0.0            0.0          1.0             0.0   \n",
      "\n",
      "      EverBenched_Yes  \n",
      "0                 0.0  \n",
      "1                 0.0  \n",
      "2                 0.0  \n",
      "3                 0.0  \n",
      "4                 1.0  \n",
      "...               ...  \n",
      "4648              0.0  \n",
      "4649              0.0  \n",
      "4650              0.0  \n",
      "4651              1.0  \n",
      "4652              1.0  \n",
      "\n",
      "[4653 rows x 14 columns]\n",
      "tar\n",
      "      LeaveOrNot\n",
      "0              0\n",
      "1              1\n",
      "2              0\n",
      "3              1\n",
      "4              1\n",
      "...          ...\n",
      "4648           0\n",
      "4649           1\n",
      "4650           1\n",
      "4651           0\n",
      "4652           0\n",
      "\n",
      "[4653 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/mlflow/data/dataset_source_registry.py:150: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment run for sklearn.ensemble.GradientBoostingClassifier with hyperparam: {'n_estimators': [100], 'learning_rate': [0.001, 0.01, 0.1, 0.5, 1.0], 'max_depth': range(1, 11), 'min_samples_split': range(2, 21), 'min_samples_leaf': range(1, 21), 'subsample': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
      "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]), 'max_features': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
      "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.12.1 of tpot is outdated. Version 0.12.2 was released Friday February 23, 2024.\n",
      "/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.8503895121540126\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.8526834201428143\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.8526834201428143\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.8526834201428143\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.8526834201428143\n",
      "\n",
      "Best pipeline: GradientBoostingClassifier(input_matrix, learning_rate=0.1, max_depth=8, max_features=0.35000000000000003, min_samples_leaf=19, min_samples_split=12, n_estimators=100, subsample=0.55)\n",
      "Pipeline(steps=[('gradientboostingclassifier',\n",
      "                 GradientBoostingClassifier(max_depth=8,\n",
      "                                            max_features=0.35000000000000003,\n",
      "                                            min_samples_leaf=19,\n",
      "                                            min_samples_split=12,\n",
      "                                            random_state=42, subsample=0.55))]) algorithms best selected by TPOT\n",
      "  score: 0.8445017182130584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.8445017182130584\n",
      "average_precision_score 0.6853586474585416\n",
      "f1_score 0.839360451875309\n",
      "precision_score 0.8452935148435857\n",
      "recall_score 0.8445017182130584\n",
      "roc_auc_score 0.8036061094032109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/mlflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 308\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Running Experiment with user configured params.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Experiment Execution with the following params:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEXPERIMENT_DETAILS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 308\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_details\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEXPERIMENT_DETAILS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtpot_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m           \u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 118\u001b[0m, in \u001b[0;36mexperiment\u001b[0;34m(exp_details, tpot_config, generations, population_size, cv, random_state, verbosity)\u001b[0m\n\u001b[1;32m    115\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m, pipeline_optimizer\u001b[38;5;241m.\u001b[39mscore(test_x, test_y))\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Log input data to MLflow run artifact.\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Registering datasets with mlflow experiment run\u001b[39;00m\n\u001b[1;32m    121\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_input(dataset, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/mlflow/tracking/fluent.py:1010\u001b[0m, in \u001b[0;36mlog_artifact\u001b[0;34m(local_path, artifact_path, run_id)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;124;03mLog a local file or directory as an artifact of the currently active run. If no run is\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;124;03mactive, this method will create a new active run.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m        mlflow.log_artifact(\"features.txt\")\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m run_id \u001b[38;5;241m=\u001b[39m run_id \u001b[38;5;129;01mor\u001b[39;00m _get_or_start_run()\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id\n\u001b[0;32m-> 1010\u001b[0m \u001b[43mMlflowClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/mlflow/tracking/client.py:1149\u001b[0m, in \u001b[0;36mMlflowClient.log_artifact\u001b[0;34m(self, run_id, local_path, artifact_path)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_artifact\u001b[39m(\u001b[38;5;28mself\u001b[39m, run_id, local_path, artifact_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;124;03m    Write a local file or directory to the remote ``artifact_uri``.\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;124;03m        is_dir: False\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1149\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/mlflow/tracking/_tracking_service/client.py:529\u001b[0m, in \u001b[0;36mTrackingServiceClient.log_artifact\u001b[0;34m(self, run_id, local_path, artifact_path)\u001b[0m\n\u001b[1;32m    527\u001b[0m     artifact_repo\u001b[38;5;241m.\u001b[39mlog_artifacts(local_path, path_name)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 529\u001b[0m     \u001b[43martifact_repo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/mlflow/store/artifact/local_artifact_repo.py:36\u001b[0m, in \u001b[0;36mLocalArtifactRepository.log_artifact\u001b[0;34m(self, local_file, artifact_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m artifact_dir \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     33\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39martifact_dir, artifact_path) \u001b[38;5;28;01mif\u001b[39;00m artifact_path \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39martifact_dir\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(artifact_dir):\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopy2(local_file, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(artifact_dir, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(local_file)))\n",
      "File \u001b[0;32m/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/mlflow/utils/file_utils.py:212\u001b[0m, in \u001b[0;36mmkdir\u001b[0;34m(root, name)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m errno\u001b[38;5;241m.\u001b[39mEEXIST \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(target):\n\u001b[0;32m--> 212\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target\n",
      "File \u001b[0;32m/packages/Python-3.8-Snowpark/707b5005-9bb5-4883-8682-105a129a2fd7/3.8/mlflow/utils/file_utils.py:209\u001b[0m, in \u001b[0;36mmkdir\u001b[0;34m(root, name)\u001b[0m\n\u001b[1;32m    207\u001b[0m target \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, name) \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m root\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m errno\u001b[38;5;241m.\u001b[39mEEXIST \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(target):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/os.py:213\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/os.py:213\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: makedirs at line 213 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/os.py:213\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/os.py:223\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/mlflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cloudpickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import (accuracy_score, average_precision_score, f1_score,\n",
    "                             precision_score, recall_score, roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow.sklearn\n",
    "from fosforio import get_local_dataframe, get_dataframe\n",
    "\n",
    "\n",
    "def experiment(exp_details, tpot_config, generations, population_size, cv, random_state, verbosity):\n",
    "    \"\"\"\n",
    "    exp_details: {\n",
    "        name: exp_name\n",
    "        id: exp_id\n",
    "        description: exp_description\n",
    "        dataset: exp_dataset\n",
    "        target_column: exp_target_column\n",
    "        algo_details: None\n",
    "    }\n",
    "    tpot_config: dict\n",
    "    generations: 0.5\n",
    "    population_size: 0.5\n",
    "    cv: 5\n",
    "    random_state: 42\n",
    "    verbosity: 2\n",
    "    \"\"\"\n",
    "    # Tracking URI set\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URL\", \"http://mlflow-server\"))\n",
    "\n",
    "    # Setting experiment name\n",
    "    mlflow.set_experiment(exp_details.get(\"name\", \"sample_experiment\"))\n",
    "\n",
    "    # Adding description to the experiment\n",
    "    tags = {'mlflow.note.content': exp_details.get(\"description\", \"sample_description\")}\n",
    "\n",
    "    # Reading input data\n",
    "    try:\n",
    "        if exp_details.get(\"dataset\").split(\".\")[-1].lower() in [\"csv\", \"tsv\", \"xlsx\", \"xls\"]:\n",
    "            # Read the input data file from /data mount attached to this notebook pod\n",
    "            input_file = \"/data/\" + exp_details.get(\"dataset\")\n",
    "            data = get_local_dataframe(input_file)\n",
    "        else:\n",
    "            # Input data is a dataset\n",
    "            data = get_dataframe(exp_details.get(\"dataset\"))\n",
    "            # Adding this to log input data in mlflow\n",
    "            input_file = '/tmp/input_data.csv'\n",
    "            data.to_csv(input_file, index=False)\n",
    "    except Exception as ex:\n",
    "        print(f\"Unable to read input dataset.\\nError: {ex}\")\n",
    "\n",
    "    # Data Preprocessing: Validating and encoding the data if required and imputing null values.\n",
    "    data = data.fillna(method='pad')  # Filling null values with the previous ones\n",
    "    data = data.fillna(method='bfill')  # Filling null value with the next ones\n",
    "    df_target, le_target, df_feature, le_dict_feature, oh_enc_feature, le_column_feature, oh_column_feature = encoding(\n",
    "        data, exp_details.get(\"target_column\"))\n",
    "\n",
    "    # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "    train_x, test_x, train_y, test_y = train_test_split(df_feature, df_target, train_size=0.75, test_size=0.25)\n",
    "\n",
    "    # Registering datasets with mlflow experiment run\n",
    "    dataset = mlflow.data.from_pandas(data, source=input_file)\n",
    "\n",
    "    for algo, hyperparam in exp_details.get(\"algo_details\").items():\n",
    "        if not hyperparam and type(hyperparam) != dict:\n",
    "            hyperparam = TPOTClassifier.default_config_dict[algo]\n",
    "        print(f\"Starting experiment run for {algo} with hyperparam: {hyperparam}\")\n",
    "        # Adding individual algorithm and its hyperparam to tpot config along with preprocessors and selectors\n",
    "        tpot_config_dict = {**tpot_config, **{algo: hyperparam}}\n",
    "        with mlflow.start_run(tags=tags):\n",
    "            pipeline_optimizer = TPOTClassifier(\n",
    "                generations=generations,\n",
    "                population_size=population_size,\n",
    "                cv=cv,\n",
    "                random_state=random_state,\n",
    "                verbosity=verbosity,\n",
    "                config_dict=tpot_config_dict\n",
    "            )\n",
    "\n",
    "            pipeline_optimizer.fit(train_x, train_y)\n",
    "\n",
    "            predicted_qualities = pipeline_optimizer.predict(test_x)\n",
    "\n",
    "            matrices = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "            print(f\"{str(pipeline_optimizer.fitted_pipeline_)} algorithms best selected by TPOT\")\n",
    "            print(f\"  score: {pipeline_optimizer.score(test_x, test_y)}\")\n",
    "\n",
    "            try:\n",
    "                # logging hyper params for the best run/pipeline chosen\n",
    "                for step, _ in pipeline_optimizer.fitted_pipeline_.named_steps.items():\n",
    "                    # Checking if the step used in TPOT pipeline is present in TPOT default config dict.\n",
    "                    # if yes, then log only the hyperparams which are present in TPOT default config and not all.\n",
    "                    step_name = [s for s in TPOTClassifier.default_config_dict.keys() if step.lower() in s.lower()]\n",
    "                    for k, v in pipeline_optimizer.fitted_pipeline_.named_steps[step].get_params().items():\n",
    "                        if step_name:\n",
    "                            if k in TPOTClassifier.default_config_dict.get(step_name[0]):\n",
    "                                mlflow.log_param(str(step) + \"_\" + str(k), v)\n",
    "                        else:\n",
    "                            mlflow.log_param(str(step) + \"_\" + str(k), v)\n",
    "            except Exception as ex:\n",
    "                print(f\"Exception occurred in logging params to mlflow.\\nex: {ex}\")\n",
    "\n",
    "            # logging model metric\n",
    "            for i in matrices:\n",
    "                if matrices[i]:\n",
    "                    mlflow.log_metric(i, matrices[i])\n",
    "                    print(i, matrices[i])\n",
    "            mlflow.log_metric(\"score\", pipeline_optimizer.score(test_x, test_y))\n",
    "\n",
    "            # Log input data to MLflow run artifact.\n",
    "            mlflow.log_artifact(input_file)\n",
    "\n",
    "            # Registering datasets with mlflow experiment run\n",
    "            mlflow.log_input(dataset, context=\"input\")\n",
    "\n",
    "            # Set custom tags\n",
    "            mlflow.set_tags({\n",
    "                \"template_id\": os.getenv(\"template_id\", \"sample_template_id\"),\n",
    "                \"notebook_name\": os.getenv(\"notebook_name\", \"sample_notebook_name\"),\n",
    "                \"algorithm\": algo,\n",
    "                \"algo_details\": exp_details.get(\"algo_details\"),\n",
    "                \"tpot_selected_algo\": str(pipeline_optimizer.fitted_pipeline_)\n",
    "            })\n",
    "\n",
    "            predictions = pipeline_optimizer.fitted_pipeline_.predict(train_x)\n",
    "            signature = infer_signature(train_x, predictions)\n",
    "\n",
    "            # Storing score function for the model\n",
    "            score_and_dump_func(\"/tmp/scoring_func\")\n",
    "            mlflow.log_artifact(\"/tmp/scoring_func\")\n",
    "\n",
    "            # Register the model\n",
    "            mlflow.sklearn.log_model(\n",
    "                pipeline_optimizer.fitted_pipeline_, \"model\",\n",
    "                registered_model_name=exp_details.get(\"name\", \"sample_experiment\"), signature=signature,\n",
    "                pip_requirements=['mlflow==2.10.0', 'sqlalchemy==1.3.5']\n",
    "            )\n",
    "\n",
    "            # Exporting the autogenerated code of tpot for best pipeline\n",
    "            pipeline_optimizer.export(f'tpot_exported_{exp_details.get(\"name\")}_{algo}.py')\n",
    "            mlflow.log_artifact(f'tpot_exported_{exp_details.get(\"name\")}_{algo}.py')\n",
    "\n",
    "\n",
    "def try_or(fn):\n",
    "    try:\n",
    "        out = fn()\n",
    "        return out\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def eval_metrics(y_actual, y_pred):\n",
    "    \"\"\"\n",
    "    :param\n",
    "    actual\n",
    "    pred\n",
    "    :returns\n",
    "    rmse, mae, r2\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"accuracy_score\": try_or(lambda: accuracy_score(y_actual, y_pred)),\n",
    "        \"average_precision_score\": try_or(lambda: average_precision_score(y_actual, y_pred)),\n",
    "        \"f1_score\": try_or(lambda: f1_score(y_actual, y_pred, average=\"weighted\", labels=np.unique(y_pred))),\n",
    "        \"precision_score\": try_or(\n",
    "            lambda: precision_score(y_actual, y_pred, average=\"weighted\", labels=np.unique(y_pred))),\n",
    "        \"recall_score\": try_or(lambda: recall_score(y_actual, y_pred, average=\"weighted\")),\n",
    "        \"roc_auc_score\": try_or(lambda: roc_auc_score(y_actual, y_pred))\n",
    "    }\n",
    "\n",
    "\n",
    "def score_and_dump_func(file_path):\n",
    "    \"\"\"\n",
    "    :param\n",
    "    file_path\n",
    "    \"\"\"\n",
    "\n",
    "    def score_func(model, request):\n",
    "        \"\"\"\n",
    "        :param\n",
    "        model\n",
    "        request\n",
    "        :returns\n",
    "        score_output\n",
    "        \"\"\"\n",
    "        # Enter your custom score function here\n",
    "\n",
    "        score_output = \"Success\"\n",
    "        return score_output\n",
    "\n",
    "    with open(file_path, \"wb\") as out:\n",
    "        cloudpickle.dump(score_func, out)\n",
    "\n",
    "\n",
    "def encoding(df, target_column):\n",
    "    \"\"\"\n",
    "    Checking whether encoding required in target and feature datasets.\n",
    "    If required, then encoding them with label and one hot encoding.\n",
    "    :param:\n",
    "    df: input dataframe\n",
    "    target_column: target column\n",
    "    :returns:\n",
    "    df_target: target dataframe\n",
    "    le_target: target label encoder object\n",
    "    df_feature: feature dataframe\n",
    "    le_dict_feature: dict of feature label encoder objects\n",
    "    oh_enc_feature: feature one hot encoder object\n",
    "    le_column_feature: list of feature label encoder columns\n",
    "    oh_column_feature: list of feature one hot encoder columns\n",
    "    \"\"\"\n",
    "    \n",
    "    print('in encoding starts')\n",
    "    df_target = df[[target_column]]\n",
    "    le_target = None\n",
    "    # Target column validation and encoding\n",
    "    if df.dtypes[target_column].name in ['object', 'bool']:\n",
    "        print(f\"target_column is of {df.dtypes[target_column].name} datatype, encoding required.\")\n",
    "        le_target = LabelEncoder()\n",
    "        df_target[target_column] = pd.DataFrame(le_target.fit_transform(df_target[target_column].astype(str)))\n",
    "        print(f\"Target column label encoded {df_target[target_column]}, object: {le_target}\")\n",
    "\n",
    "    # Feature column validation and encoding\n",
    "    df_feature = df.drop(target_column, axis=1)\n",
    "    non_numeric_cols = df_feature.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "    le_dict_feature = {}\n",
    "    le_column_feature = []\n",
    "    oh_column_feature = []\n",
    "    oh_enc_feature = None\n",
    "    if len(non_numeric_cols) >= 1:\n",
    "        print(f\"{non_numeric_cols} columns are non numeric in feature dataset, encoding required.\")\n",
    "        for col in non_numeric_cols:\n",
    "            if df_feature[col].nunique() >= 10:\n",
    "                le_column_feature.append(col)\n",
    "            else:\n",
    "                oh_column_feature.append(col)\n",
    "\n",
    "        print(f\"Columns identified to be encoded with label encoder: {le_column_feature}\\n\"\n",
    "              f\"Columns identified to be encoded with one hot encoder: {oh_column_feature}\")\n",
    "\n",
    "        # columns to be label encoded\n",
    "        if len(le_column_feature) == 0:\n",
    "            df_feature = df_feature\n",
    "        else:\n",
    "            for col in le_column_feature:\n",
    "                le_dict_feature[col] = LabelEncoder()\n",
    "                df_feature[col] = le_dict_feature[col].fit_transform(df_feature[col].astype(str))\n",
    "                print(f\"{col} column label encoded {df_feature[col]}, object: {le_dict_feature[col]}\")\n",
    "\n",
    "        # columns to be one hot encoded\n",
    "        if len(oh_column_feature) == 0:\n",
    "            df_feature = df_feature\n",
    "        else:\n",
    "            unique_combinations = pd.get_dummies(df_feature[oh_column_feature])\n",
    "            unique_combinations_list = unique_combinations.columns.tolist()\n",
    "            oh_enc_feature = OneHotEncoder()\n",
    "            oh_encoded_array = oh_enc_feature.fit_transform(df_feature[oh_column_feature]).toarray() if len(\n",
    "                oh_column_feature) > 1 else oh_enc_feature.fit_transform(df_feature[oh_column_feature]).toarray()\n",
    "            df_oh_enc = pd.DataFrame(oh_encoded_array, columns=unique_combinations_list)\n",
    "            df_feature = df_feature.drop(columns=oh_column_feature)\n",
    "            df_feature = df_feature.join(df_oh_enc)\n",
    "            print(f\"new one hot encoded df: {oh_encoded_array}\\n\"\n",
    "                  f\"one hot encoder object: {oh_enc_feature}\\n\")\n",
    "        print(f\"final feature df created: {df_feature}\")\n",
    "        print(' encoding ends')\n",
    "\n",
    "        print(df_feature)\n",
    "        df_feature.to_csv('/data/finalOutput.csv')\n",
    "        print('tar')\n",
    "        print(df_target)\n",
    "        \n",
    "    return df_target, le_target, df_feature, le_dict_feature, oh_enc_feature, le_column_feature, oh_column_feature\n",
    "\n",
    "\n",
    "# Adding Preprocessors and Selectors with tpot default hyperparameter tuning\n",
    "preprocessors_selectors = [\n",
    "    # Preprocessors\n",
    "    \"sklearn.preprocessing.Binarizer\",\n",
    "    \"sklearn.decomposition.FastICA\",\n",
    "    \"sklearn.cluster.FeatureAgglomeration\",\n",
    "    \"sklearn.preprocessing.MaxAbsScaler\",\n",
    "    \"sklearn.preprocessing.MinMaxScaler\",\n",
    "    \"sklearn.preprocessing.Normalizer\",\n",
    "    \"sklearn.kernel_approximation.Nystroem\",\n",
    "    \"sklearn.decomposition.PCA\",\n",
    "    \"sklearn.preprocessing.PolynomialFeatures\",\n",
    "    \"sklearn.kernel_approximation.RBFSampler\",\n",
    "    \"sklearn.preprocessing.RobustScaler\",\n",
    "    \"sklearn.preprocessing.StandardScaler\",\n",
    "    \"tpot.builtins.ZeroCount\",\n",
    "    \"tpot.builtins.OneHotEncoder\",\n",
    "    # Selectors\n",
    "    \"sklearn.feature_selection.SelectFwe\",\n",
    "    \"sklearn.feature_selection.SelectPercentile\",\n",
    "    \"sklearn.feature_selection.VarianceThreshold\",\n",
    "    \"sklearn.feature_selection.RFE\",\n",
    "    \"sklearn.feature_selection.SelectFromModel\"\n",
    "]\n",
    "tpot_config = {key: TPOTClassifier.default_config_dict[key] for key in preprocessors_selectors}\n",
    "\n",
    "# Running Experiment with user configured params.\n",
    "print(f\"Starting Experiment Execution with the following params:\\n{os.getenv('EXPERIMENT_DETAILS')}\\n\")\n",
    "experiment(exp_details=json.loads(os.getenv(\"EXPERIMENT_DETAILS\")), tpot_config=tpot_config, generations=5,\n",
    "           population_size=20, cv=5, random_state=42, verbosity=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b69d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
